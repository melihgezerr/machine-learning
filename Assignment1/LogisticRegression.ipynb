{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2a64b2d-3350-4d72-9aa8-cb13c4418b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T19:07:40.584018Z",
     "start_time": "2024-03-04T19:07:40.551985Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arff_file_path = './Rice_Cammeo_Osmancik.arff'\n",
    "\n",
    "data, meta = arff.loadarff(arff_file_path)      # loading the data\n",
    "\n",
    "df = pd.DataFrame(data)                         # put the data into data frame\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)   # shuffle the data\n",
    "\n",
    "features = df.drop(\"Class\", axis=1)             # extract features\n",
    "label = df[\"Class\"]                             # extract label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.16, random_state=42) # split into training and test set\n",
    "\n",
    "# normalize(using min-max normalization) both training and test set based on the training set max and min values\n",
    "min_vals = X_train.min()\n",
    "max_vals = X_train.max()\n",
    "column_to_be_normalized = ['Area', 'Perimeter','Major_Axis_Length','Minor_Axis_Length','Convex_Area']\n",
    "\n",
    "for col in X_train:\n",
    "    if col in column_to_be_normalized:\n",
    "        X_train[col] = (X_train[col] - min_vals[col]) / (max_vals[col] - min_vals[col])\n",
    "\n",
    "for col in X_test:\n",
    "    if col in column_to_be_normalized:\n",
    "        X_test[col] = (X_test[col] - min_vals[col]) / (max_vals[col] - min_vals[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8749f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with gradient descent\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, data, labels,learning_rate, regularized = False, reg_param = 0):\n",
    "        self.weights = np.random.rand(data.shape[1])\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.N = len(data)\n",
    "        self.threshold = 0.00004\n",
    "        self.regularized = regularized\n",
    "        self.reg_param = reg_param\n",
    "\n",
    "    def fit(self):\n",
    "        prev_weights = None\n",
    "\n",
    "        #print(f\"Lambda : {self.reg_param}\")\n",
    "\n",
    "        # apply gradient descent until no improvement on error func\n",
    "        while not self.termination_condition(self.weights,prev_weights):\n",
    "            prev_weights = self.weights + np.zeros(self.weights.shape[0])   # avoid pass by reference\n",
    "            self.predict()\n",
    "         #   print(self.calculate_error(self.weights))\n",
    "            \n",
    "        return self.weights\n",
    "\n",
    "    \n",
    "    def calculate_error(self,weights):\n",
    "        total_sum = 0\n",
    "        for i in range(self.N):\n",
    "            total_sum = total_sum + np.log(1 + np.exp(-self.labels[i]*(weights @ self.data[i])))\n",
    "        total_sum =  (1/self.N)*total_sum\n",
    "\n",
    "        # regularization\n",
    "        if self.regularized == True:\n",
    "            total_sum += self.reg_param * (self.weights @ self.weights)\n",
    "\n",
    "        return total_sum\n",
    "\n",
    "    def termination_condition(self,curr_weights, prev_weights = None):\n",
    "           if (prev_weights is not None) and abs(self.calculate_error(curr_weights) - self.calculate_error(prev_weights))<self.threshold:\n",
    "                return True\n",
    "           return False\n",
    "    \n",
    "    # compute gradient and take a step to the opposite direction and update weights accordingly\n",
    "    def predict(self):\n",
    "        gradient = 0\n",
    "        for i in range(self.N):\n",
    "            gradient = gradient + (self.labels[i]*self.data[i])/(1 + np.exp(self.labels[i]*(self.weights @ self.data[i])))\n",
    "        gradient =  (-1/self.N)*gradient\n",
    "\n",
    "        # derivative comes from regularization term\n",
    "        if self.reg_param == True:\n",
    "            gradient += 2 * self.reg_param * self.weights\n",
    "\n",
    "        direction = -gradient\n",
    "        self.weights = self.weights + self.learning_rate*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d885b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with stochastic gradient descent\n",
    "\n",
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, data, labels,learning_rate):\n",
    "        self.weights = np.random.rand(data.shape[1])\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.N = len(data)\n",
    "        self.threshold = 0.00004\n",
    "        self.error_data = list()\n",
    "\n",
    "    def fit(self):\n",
    "        prev_weights = None\n",
    "        while True:\n",
    "            random_index = None\n",
    "            for round_count in range(self.N):\n",
    "                random_index = np.random.randint(0, self.N)\n",
    "                prev_weights = self.weights + np.zeros(self.weights.shape[0])\n",
    "                self.predict(random_index)\n",
    "                random_index = np.random.randint(0, self.N)\n",
    "                self.error_data.append(self.calculate_error(self.weights,random_index))\n",
    "            if random_index is not None and self.termination_condition(self.weights,prev_weights,random_index):\n",
    "                break\n",
    "                    \n",
    "    def calculate_error(self,weights,random_index):\n",
    "        return np.log(1 + np.exp(-self.labels[random_index]*(weights @ self.data[random_index])))\n",
    "\n",
    "        \n",
    "    def termination_condition(self,curr_weights,prev_weights,random_index):\n",
    "           if (prev_weights is not None) and abs(self.calculate_error(curr_weights,random_index) - self.calculate_error(prev_weights,random_index))<self.threshold:\n",
    "                return True\n",
    "           return False\n",
    "           \n",
    "           \n",
    "    def predict(self,random_index):\n",
    "        direction = (self.labels[random_index]*self.data[random_index])/(1 + np.exp(self.labels[random_index]*(self.weights @ self.data[random_index])))\n",
    "        self.weights = self.weights + self.learning_rate*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "19536431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(test_data, test_labels, weights):\n",
    "    corrects = 0\n",
    "    for i in range(len(test_data)):\n",
    "        s = weights @ test_data[i]\n",
    "        output = 1 / (1 + np.exp(-s))\n",
    "\n",
    "        if (output > 0.5 and test_labels[i] == 1) or (output < 0.5 and test_labels[i] == -1):\n",
    "            corrects += 1\n",
    "            \n",
    "    acc = corrects / len(test_data)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9fc01248-5c39-47cc-b0b6-0981fc2b10b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T19:08:21.052331Z",
     "start_time": "2024-03-04T19:08:20.917928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen lambda : 0.1\n",
      "lambda acc : 0.18000000000000002\n",
      "lambda acc : 0.35468750000000004\n",
      "lambda acc : 0.5375000000000001\n",
      "lambda acc : 0.7187500000000001\n",
      "lambda acc : 0.8987500000000002\n",
      "-------------\n",
      "chosen lambda : 0.25\n",
      "lambda acc : 0.1809375\n",
      "lambda acc : 0.35968750000000005\n",
      "lambda acc : 0.5440625000000001\n",
      "lambda acc : 0.7278125000000001\n",
      "lambda acc : 0.9109375000000001\n",
      "-------------\n",
      "chosen lambda : 0.5\n",
      "lambda acc : 0.1809375\n",
      "lambda acc : 0.3615625\n",
      "lambda acc : 0.54125\n",
      "lambda acc : 0.725\n",
      "lambda acc : 0.9059375\n",
      "-------------\n",
      "chosen lambda : 0.75\n",
      "lambda acc : 0.180625\n",
      "lambda acc : 0.36\n",
      "lambda acc : 0.53625\n",
      "lambda acc : 0.7153125\n",
      "lambda acc : 0.89625\n",
      "-------------\n",
      "chosen lambda : 0.9\n",
      "lambda acc : 0.180625\n",
      "lambda acc : 0.36562500000000003\n",
      "lambda acc : 0.5487500000000001\n",
      "lambda acc : 0.7325\n",
      "lambda acc : 0.9131250000000001\n",
      "-------------\n",
      "[0.1, 0.25, 0.5, 0.75, 0.9]\n",
      "[0.8987500000000002, 0.9109375000000001, 0.9059375, 0.89625, 0.9131250000000001]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    tranformed_y_train = np.array([-1 if x == b'Cammeo' else 1 for x in y_train.values])\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    lambdas = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "    lambda_accuracies = []\n",
    "\n",
    "    for cross in range(5):\n",
    "        reg_param = lambdas[cross] \n",
    "        print(f\"chosen lambda : {reg_param}\")\n",
    "        lambda_acc = 0\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            # Split the data into training and validation sets\n",
    "            X_cross_train, X_val = X_train.values[train_index], X_train.values[val_index]\n",
    "            y_cross_train, y_val = tranformed_y_train[train_index], tranformed_y_train[val_index]\n",
    "\n",
    "            logistic_regression = LogisticRegressionGD(X_cross_train, y_cross_train, 0.005, regularized=True, reg_param=reg_param)\n",
    "            learned_weights = logistic_regression.fit()\n",
    "\n",
    "            lambda_acc += evalute(X_val, y_val, learned_weights) * (1 / len(lambdas))\n",
    "\n",
    "            print(f\"lambda acc : {lambda_acc}\")\n",
    "        print(\"-------------\")\n",
    "        lambda_accuracies.append(lambda_acc)\n",
    "\n",
    "    print(lambdas)\n",
    "    print(lambda_accuracies)\n",
    "\n",
    "    #errors = logistic_regression.error_data\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    #plt.plot(range(100), errors[0:100], color='red', linestyle='-', linewidth=2, label='Straight line')\n",
    "    #plt.xlabel('Iteration')\n",
    "    #plt.ylabel('Error')\n",
    "    #plt.title('Error Plot (for the first 100 random data points)')\n",
    "    #plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
